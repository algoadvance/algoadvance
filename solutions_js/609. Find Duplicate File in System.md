---
layout: page
title: leetcode 609. Find Duplicate File in System
permalink: /s609-js
---
[Leetcode 609. Find Duplicate File in System](https://algoadvance.github.io/algoadvance/l609)
### Problem Statement:

Given a list of directory paths, where each path contains several files, your task is to find all the duplicate files in the system. A file is considered a duplicate if at least one other file in the system has the same content.

For example, suppose a directory contains the following entries:
```
"root/a 1.txt(abcd) 2.txt(efgh)"
"root/c 3.txt(abcd)"
"root/c/d 4.txt(efgh)"
"root 4.txt(efgh)"
```
Here, `1.txt` (in `root/a`) and `3.txt` (in `root/c`) have the same content i.e., `(abcd)` and `2.txt` (in `root/a`), `4.txt` (in `root/c/d`), and `4.txt` (in `root`) also have the same content i.e., `(efgh)`.

To clarify the format:
- The input is a list of strings.
- Each string starts with a directory path, followed by one or more file entries.
- Each file entry consists of the filename and the file content (in parentheses).

You need to output a list of lists, where each list contains the file paths for files containing the same content.

### Clarifying Questions:
1. Can the paths include special characters or spaces that need specific handling?
2. Is it guaranteed that file content (in parentheses) will not contain parentheses within them?
3. Should each sublist in the output contain paths in any specific order?

### Strategy:
1. Use a dictionary to map file content to a list of file paths.
2. Iterate through each directory path provided in the input.
3. For each directory path, extract the directory and individual file entries (filename and content).
4. Update the dictionary with content as the key and append corresponding paths to it.
5. Once all files are processed, gather the lists of file paths that have more than one entry and return them.

### Time Complexity:
- Extracting filenames and content from the input list is linear in terms of the total length of all strings in the input.
- Final processing of the dictionary to generate the output list is also linear.
So, the overall time complexity is O(n), where n is the total length of all directory paths combined.

### Code:

```javascript
function findDuplicate(paths) {
    let contentToFilePaths = {};

    for (let path of paths) {
        let parts = path.split(' ');
        let dir = parts[0];

        for (let i = 1; i < parts.length; i++) {
            let fileAndContent = parts[i];
            let openParen = fileAndContent.indexOf('(');
            let closeParen = fileAndContent.indexOf(')');
            
            let fileName = fileAndContent.substring(0, openParen);
            let content = fileAndContent.substring(openParen + 1, closeParen);
            
            let filePath = dir + '/' + fileName;

            if (!contentToFilePaths[content]) {
                contentToFilePaths[content] = [];
            }
            contentToFilePaths[content].push(filePath);
        }
    }

    let duplicates = [];
    for (let content in contentToFilePaths) {
        if (contentToFilePaths[content].length > 1) {
            duplicates.push(contentToFilePaths[content]);
        }
    }

    return duplicates;
}
```

### Explanation:
1. `contentToFilePaths` is a dictionary to store content and corresponding file paths.
2. Loop through each directory entry in the `paths` list.
3. Split each directory entry into the directory path and its files.
4. For each file, extract the filename and content using string manipulation.
5. Append file paths to the dictionary under the appropriate content key.
6. Collect and return only those lists with more than one file path as the result.


### Cut your prep time in half and DOMINATE your interview with [AlgoAdvance AI](https://algoAdvance.com)

- Got blindsided by a question you didn't expect?

- Spend too much time studying?

- Or simply don't have the time to go over all 3000 questions?

