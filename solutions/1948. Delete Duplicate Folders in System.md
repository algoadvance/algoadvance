---
layout: page
title:  Delete Duplicate Folders in System-out
permalink: /s1948
---

Given a file system represented as a dictionary structure where each key is a folder and the value is either another dictionary representing subfolders or a value indicating the absence of further subfolders, the goal is to delete all duplicate folders and their subfolders recursively. To identify duplicates, folders are considered equivalent if they contain identical subfolders with identical structures and names.

You need to implement a function `deleteDuplicateFolder(input: List[List[str]]) -> List[List[str]]` that deletes such duplicates and returns the remaining folder structure.

### Example:
#### Input:
```
[
    ["a","x"],
    ["a","y"],
    ["b","x"],
    ["b","y"],
    ["z"]
]
```

#### Output:
```
[
    ["z"]
]
```

## Clarifying Questions:

1. **Root Folder**: Should the provided input always be considered relative to an implicit root folder?
2. **Empty Input**: What should be the behavior when given an empty input?
3. **Output Format**: Is the expected output always a list of paths?

## Strategy:

1. **Trie Creation**:
    - Convert the given list of paths into a Trie-like structure (nested dictionaries) to facilitate easy traversal and comparison.

2. **Canonical Form Calculation**:
    - Traverse the trie to calculate a canonical string representation for each folder. This will help in identifying duplicates. 

3. **Duplicate Detection**:
    - Use a dictionary to keep track of these canonical forms and the paths associated with them to detect duplicates.

4. **Deletion of Duplicates**:
    - Remove folders and their subfolders if they were marked as duplicates.

5. **Convert Back to Paths**:
    - Convert the remaining Trie structure back to the list of paths format.

## Code:

```python
from collections import defaultdict

def path_to_trie(paths):
    trie = {}
    for path in paths:
        current_level = trie
        for folder in path:
            if folder not in current_level:
                current_level[folder] = {}
            current_level = current_level[folder]
    return trie

def get_canonical_form(node):
    if not node:
        return ""
    canonical_parts = []
    for key in sorted(node.keys()):
        canonical_parts.append(key + "(" + get_canonical_form(node[key]) + ")")
    return "".join(canonical_parts)

def mark_duplicates(trie):
    canonical_dict = defaultdict(list)
    
    def dfs(node, path):
        if not node:
            return ""
        canonical_form = get_canonical_form(node)
        canonical_dict[canonical_form].append(path)
        for folder in node:
            dfs(node[folder], path + [folder])
        return canonical_form

    dfs(trie, [])
    
    duplicates = set()
    for canonical_form, paths in canonical_dict.items():
        if len(paths) > 1:
            for path in paths:
                duplicates.add(tuple(path))
    
    return duplicates

def remove_duplicates(trie, path, duplicates):
    keys_to_delete = []
    for folder in trie:
        full_path = path + [folder]
        if tuple(full_path) in duplicates:
            keys_to_delete.append(folder)
        else:
            remove_duplicates(trie[folder], full_path, duplicates)
    
    for key in keys_to_delete:
        del trie[key]

def trie_to_paths(trie, base_path):
    paths = []
    for k, v in trie.items():
        current_path = base_path + [k]
        if v: 
            paths.extend(trie_to_paths(v, current_path))
        else:
            paths.append(current_path)
    return paths

def deleteDuplicateFolder(paths):
    trie = path_to_trie(paths)
    duplicates = mark_duplicates(trie)
    remove_duplicates(trie, [], duplicates)
    return trie_to_paths(trie, [])

# Test
input_paths = [["a","x"], ["a","y"], ["b","x"], ["b","y"], ["z"]]
print(deleteDuplicateFolder(input_paths))
```

## Time Complexity:

- **Trie Creation**: \(O(N \cdot M)\) where \(N\) is the number of paths and \(M\) is the average length of each path.
- **Canonical Form Calculation**: \(O(V + E)\) where \(V\) is the number of vertices (folders) and \(E\) is the number of edges (subfolders).
- **Duplicate Detection**: \(O(V \cdot \log V)\) due to sorting of subfolder names.
- **Final Path Extraction**: \(O(V)\).

Overall, the time complexity is \(O(N \cdot M + V \cdot \log V)\). The solution is efficient given the typical constraints in such problems.


### Try our interview co-pilot at [AlgoAdvance.com](https://algoAdvance.com)

- Got blindsided by a question you didn't expect?

- Spend too much time studying?

- Or simply don't have the time to go over all 3000 questions?

